{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for results concatenation and preprocessing\n",
    "\n",
    "Important note: during research we decided to extend analysis for AND protocol,\n",
    "therefore you can find two types of `csv` files for each method - one that has\n",
    "been calculated originally (`results.csv`) and another from repeated experiments\n",
    "(`results_recalc_and.csv`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random seed selector processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_path_1 = Path(\"_experiments/all_methods/random/results.csv\")\n",
    "random_df_1 = pd.read_csv(random_path_1).drop(\"Unnamed: 0\", axis=1)\n",
    "print(len(random_df_1))\n",
    "random_df_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_path_2 = Path(\"_experiments/all_methods/random/results_recalc_and.csv\")\n",
    "random_df_2 = pd.read_csv(random_path_2).drop(\"Unnamed: 0\", axis=1)\n",
    "print(len(random_df_2))\n",
    "random_df_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_path_3 = Path(\"_experiments/all_methods/random/results_recalc_single_layer.csv\")\n",
    "random_df_3 = pd.read_csv(random_path_3).drop(\"Unnamed: 0\", axis=1)\n",
    "print(len(random_df_3))\n",
    "random_df_3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_path_4 = Path(\"_experiments/all_methods/random/results_recalc_single_layer_and.csv\")\n",
    "random_df_4 = pd.read_csv(random_path_4).drop(\"Unnamed: 0\", axis=1)\n",
    "print(len(random_df_4))\n",
    "random_df_4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_df = pd.concat(\n",
    "    [random_df_1, random_df_2, random_df_3, random_df_4], ignore_index=True\n",
    ")\n",
    "print(\n",
    "    len(random_df),\n",
    "    len(random_df_1) + len(random_df_2) + len(random_df_3) + len(random_df_4)\n",
    ")\n",
    "random_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_params = set(random_df.columns)\n",
    "experiment_params.remove(\"repetition_run\")\n",
    "experiment_params.remove(\"gain\")\n",
    "experiment_params.remove(\"diffusion_len\")\n",
    "experiment_params.remove(\"active_actors_prct\")\n",
    "experiment_params.remove(\"seed_actors_prct\")\n",
    "\n",
    "experiment_metrics = set(random_df.columns).difference(experiment_params)\n",
    "\n",
    "experiment_params = list(experiment_params)\n",
    "experiment_metrics = list(experiment_metrics)\n",
    "\n",
    "print(f\"Columns that are multi-indices: {experiment_params}\")\n",
    "print(f\"Columns that have been left: {experiment_metrics}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reindexed_df = random_df.set_index(experiment_params)\n",
    "reindexed_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "averaged_random_df = pd.DataFrame()\n",
    "for metric in experiment_metrics:\n",
    "    avg = reindexed_df.groupby(reindexed_df.index)[str(metric)].mean()\n",
    "    averaged_random_df = pd.concat([averaged_random_df, avg], axis=1)\n",
    "averaged_random_df.index = pd.MultiIndex.from_tuples(averaged_random_df.index, names=experiment_params)\n",
    "averaged_random_df = averaged_random_df.reset_index()\n",
    "\n",
    "averaged_random_df[\"selection_metric\"] = \"random\"\n",
    "\n",
    "print(f\"Length of averaged random dataframe: {len(averaged_random_df)}\")\n",
    "averaged_random_df.head()\n",
    "# averaged_random_df.to_csv(\"random_avg.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(reindexed_df) / max(reindexed_df[\"repetition_run\"]) == len(averaged_random_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Greedy seed selector processing\n",
    "\n",
    "Postprocessing of greedy is more complicated - first we need to remove \n",
    "duplicated computations for enhanced evaluation of AND, because we had to \n",
    "repeat all computations that have been done in the first experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "greedy_path_1 = Path(\"_experiments/all_methods/greedy/results.csv\")\n",
    "greedy_df_1 = pd.read_csv(greedy_path_1).drop(\"Unnamed: 0\", axis=1)\n",
    "greedy_df_1 = greedy_df_1.drop(\n",
    "    index=greedy_df_1.loc[greedy_df_1[\"protocol\"] == \"AND\"].index\n",
    ")\n",
    "\n",
    "greedy_path_2 = Path(\"_experiments/all_methods/greedy/results_recalc_and.csv\")\n",
    "greedy_df_2 = pd.read_csv(greedy_path_2).drop(\"Unnamed: 0\", axis=1)\n",
    "\n",
    "# for eu_transportation we conucted experiments with s. budget up to 40% both \n",
    "# for OR and AND protocols, but in all another s.s. medhods we have budgets for \n",
    "# OR up to 30%, hence we need to drop redundant columns\n",
    "greedy_path_3 = Path(\"_experiments/all_methods/greedy/results_recalc_eutr.csv\")\n",
    "greedy_df_3 = pd.read_csv(greedy_path_3).drop(\"Unnamed: 0\", axis=1)\n",
    "eutr_to_drop_cols = greedy_df_3.loc[\n",
    "    (greedy_df_3[\"protocol\"] == \"OR\") & (greedy_df_3[\"seeding_budget\"] >= 31)\n",
    "]\n",
    "greedy_df_3 = greedy_df_3.drop(eutr_to_drop_cols.index)\n",
    "\n",
    "greedy_df = pd.concat(\n",
    "    [greedy_df_1, greedy_df_2, greedy_df_3], ignore_index=True\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"Length of concatenated greedy df: {len(greedy_df)}\\\n",
    "    {len(greedy_df_1) + len(greedy_df_2) + len(greedy_df_3)}\"\n",
    ")\n",
    "greedy_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to remove rows which have `seed_actors_prct` inconsistent with \n",
    "reference data. In greedy method we cannot jump e.g. directly from 10% seeding\n",
    "budget to 15% - we need to ewaluate all sizes between that range. For other \n",
    "methods we could do that, hence to obtain consistent visualisation we will \n",
    "remove these intermediate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Length of raw dataframe: {len(greedy_df)}\")\n",
    "reference_df = averaged_random_df\n",
    "\n",
    "for net in greedy_df[\"network\"].unique():\n",
    "\n",
    "    # take unique seed_actors_prct vals for all ssm except greedy given the net\n",
    "    allowed_values = reference_df.loc[\n",
    "        reference_df[\"network\"] == net\n",
    "    ][\"seed_actors_prct\"].unique()\n",
    "\n",
    "    # take all rows where ssm is greedy and seed_actors_prct is not a good val\n",
    "    greedy_rows_to_be_dropped = greedy_df.loc[\n",
    "        (greedy_df[\"network\"] == net) &\n",
    "        (~greedy_df[\"seed_actors_prct\"].round(2).isin(allowed_values.round(2)))\n",
    "    ]\n",
    "\n",
    "    print(f\"Removing {len(greedy_rows_to_be_dropped)} rows for net: {net}\")\n",
    "    greedy_df = greedy_df.drop(greedy_rows_to_be_dropped.index)\n",
    "\n",
    "greedy_df[\"selection_metric\"] = \"greedy\"\n",
    "\n",
    "print(f\"Length of processed dataframe: {len(greedy_df)}\")\n",
    "greedy_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next thing is alignment or used seeding budgets. For small networks seeding\n",
    "budgets like 1% and 2% actors of the network can be the same. Greedy ignores \n",
    "that - it takes 1 actor, 2 actors, 3 actors and so on. We need therefore to\n",
    "align these records to obtain the same seeding bugdets as for another methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_greedy_df = pd.DataFrame(columns=greedy_df.columns)\n",
    "\n",
    "for _, greedy_row in greedy_df.iterrows():\n",
    "    \n",
    "    reference_rows = reference_df.loc[\n",
    "        (reference_df[\"network\"] == greedy_row[\"network\"]) &\n",
    "        (reference_df[\"protocol\"] == greedy_row[\"protocol\"]) &\n",
    "        (reference_df[\"mi_value\"].round(2) == round(greedy_row[\"mi_value\"], 2)) &\n",
    "        (reference_df[\"seed_actors_prct\"].round(2) == round(greedy_row[\"seed_actors_prct\"], 2))\n",
    "    ]\n",
    "\n",
    "    if len(reference_rows) < 1:\n",
    "        print(greedy_row)\n",
    "        raise ValueError(\"Inconsistency in data!\")\n",
    "\n",
    "    for __, reference_row in reference_rows.iterrows():\n",
    "\n",
    "        reference_dict = reference_row.to_dict()\n",
    "        reference_dict[\"repetition_run\"] = greedy_row[\"repetition_run\"]\n",
    "        reference_dict[\"diffusion_len\"] = greedy_row[\"diffusion_len\"]\n",
    "        reference_dict[\"active_actors_prct\"] = greedy_row[\"active_actors_prct\"]\n",
    "        reference_dict[\"gain\"] = greedy_row[\"gain\"]\n",
    "        reference_dict[\"selection_metric\"] = greedy_row[\"selection_metric\"]\n",
    "\n",
    "        final_greedy_df = pd.concat(\n",
    "            [final_greedy_df, pd.DataFrame.from_records([reference_dict])]\n",
    "        )\n",
    "\n",
    "del greedy_df\n",
    "\n",
    "print(f\"Length of final greedy dataframe: {len(final_greedy_df)}\")\n",
    "final_greedy_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing of another metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = Path(\"_experiments/all_methods\")\n",
    "final_path = root_path.joinpath(\"all_results.csv\")\n",
    "if final_path.exists():\n",
    "    final_path.unlink()\n",
    "experiments = [*root_path.glob(\"*\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_csv(metric_path):\n",
    "    print(f\"\\n\\n\\nPreparing results for: {metric_path}\")\n",
    "    partial_results = []\n",
    "    for pr_path in [\"results.csv\", \"results_recalc_and.csv\", \"results_recalc_single_layer.csv\", \"results_recalc_single_layer_and.csv\"]:\n",
    "        try:\n",
    "            pr_df = pd.read_csv(metric_path.joinpath(pr_path), index_col=0)\n",
    "            partial_results.append(pr_df)\n",
    "        except:\n",
    "            print(f\"File {pr_path} doesn't exist for {metric_path}\")\n",
    "    df = pd.concat(partial_results, ignore_index=True)\n",
    "    df[\"selection_metric\"] = metric_path.stem  \n",
    "    print(f\"{metric_path.stem}: \" + \"+\".join([str(len(pr_df)) for pr_df in partial_results]) + f\"={len(df)}\")\n",
    "    assert len(df) == sum([len(pr_df) for pr_df in partial_results])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_dfs = [\n",
    "    prepare_csv(e_name) for e_name in experiments if \n",
    "    (\"random\" not in str(e_name) and \"greedy\" not in str(e_name))\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving final dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_dfs.append(averaged_random_df)\n",
    "exp_dfs.append(final_greedy_df)\n",
    "final_df = pd.concat(exp_dfs)\n",
    "final_df = final_df.drop(\"repetition_run\", axis=1)\n",
    "final_df = final_df.reset_index().drop(\"index\", axis=1)\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_ssm = {\n",
    "    \"cbim_and\": \"cbim\",\n",
    "    \"cbim_or\": \"cbim\",\n",
    "    \"cim_and\": \"cim\",\n",
    "    \"cim_or\": \"cim\",\n",
    "    # \"degree_centrality\": \"degree_centrality\",\n",
    "    \"degree_centrality_discount_and\": \"degree_centrality_discount\",\n",
    "    \"degree_centrality_discount_or\": \"degree_centrality_discount\",\n",
    "    # \"greedy\": \"greedy\",\n",
    "    # \"k_shell\": \"k_shell\",\n",
    "    # \"k_shell_mln\": \"k_shell_mln\",\n",
    "    \"kpp_shell_and\": \"kpp_shell\",\n",
    "    \"kpp_shell_or\": \"kpp_shell\",\n",
    "    # \"neighbourhood_2_hop_size\": \"neighbourhood_2_hop_size\",\n",
    "    # \"neighbourhood_size\": \"neighbourhood_size\",\n",
    "    \"neighbourhood_size_discount_and\": \"neighbourhood_size_discount\",\n",
    "    \"neighbourhood_size_discount_or\": \"neighbourhood_size_discount\",\n",
    "    # \"page_rank\": \"page_rank\",\n",
    "    # \"page_rank_mln\": \"page_rank_mln\",\n",
    "    # \"random\": \"random\",\n",
    "    # \"vote_rank\": \"vote_rank\",\n",
    "    # \"vote_rank_mln\": \"vote_rank_mln\",\n",
    "}\n",
    "\n",
    "for ssm_raw, ssm_target in mapping_ssm.items():\n",
    "    final_df[\"selection_metric\"] = final_df[\"selection_metric\"].replace(ssm_raw, ssm_target)\n",
    "\n",
    "print(final_df[\"selection_metric\"].unique())\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv(root_path.joinpath(\"all_results.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing experiments for top methods "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = Path(\"_experiments/top_methods\")\n",
    "final_path = root_path.joinpath(\"all_results.csv\")\n",
    "if final_path.exists():\n",
    "    final_path.unlink()\n",
    "experiments = [*root_path.glob(\"*\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_csv(metric_path):\n",
    "    print(f\"\\nPreparing results for: {metric_path}\")\n",
    "    df = pd.read_csv(metric_path.joinpath(\"results.csv\"), index_col=0)\n",
    "    df[\"selection_metric\"] = metric_path.stem  \n",
    "    print(f\"{metric_path.stem}: {len(df)}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_dfs = [prepare_csv(e_name) for e_name in experiments]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.concat(exp_dfs)\n",
    "final_df = final_df.drop(\"repetition_run\", axis=1)\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_ssm = {\n",
    "    \"degree_centrality_discount_and\": \"degree_centrality_discount\",\n",
    "    \"degree_centrality_discount_or\": \"degree_centrality_discount\",\n",
    "    \"neighbourhood_size_discount_and\": \"neighbourhood_size_discount\",\n",
    "    \"neighbourhood_size_discount_or\": \"neighbourhood_size_discount\",\n",
    "    \"page_rank_and\": \"page_rank\",\n",
    "    \"page_rank_or\": \"page_rank\",\n",
    "    \"vote_rank_and\": \"vote_rank\",\n",
    "    \"vote_rank_or\": \"vote_rank\",\n",
    "    \"vote_rank_mln_and\": \"vote_rank_mln\",\n",
    "    \"vote_rank_mln_or\": \"vote_rank_mln\",\n",
    "}\n",
    "\n",
    "for ssm_raw, ssm_target in mapping_ssm.items():\n",
    "    final_df[\"selection_metric\"] = final_df[\"selection_metric\"].replace(ssm_raw, ssm_target)\n",
    "\n",
    "print(final_df[\"selection_metric\"].unique())\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv(root_path.joinpath(\"all_results.csv\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ltm-seeding-mln",
   "language": "python",
   "name": "ltm-seeding-mln"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "370e4dcd90c6b278f2eaed32c70bf26cf961be4169d33535105c1f563db5caee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
